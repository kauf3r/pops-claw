---
phase: 26-agent-observability
plan: 02
type: execute
wave: 2
depends_on:
  - 26-01
files_modified:
  - ~/.openclaw/cron/jobs.json
  - ~/clawd/agents/main/observability.db
  - ~/clawd/agents/main/OBSERVABILITY.md
autonomous: true
requirements:
  - OBS-02
  - OBS-03

must_haves:
  truths:
    - "Bob can report per-agent token usage, model distribution, turn counts, cost, latency, and errors for the last 24 hours"
    - "Morning briefing includes an Agent Observability section with per-agent summary table"
    - "Anomaly detection flags warning at 2x rolling average and critical at 4x or repeated errors"
    - "Zero-activity agents are flagged as warnings when they had daily activity in the prior 7 days"
    - "Cold-start period shows 'collecting baseline (day N/7)' instead of false anomaly flags"
    - "Rate limit proximity is estimated from cumulative token volume against known tier limits"
    - "Data older than 90 days is cleaned up automatically"
  artifacts:
    - path: "~/clawd/agents/main/OBSERVABILITY.md"
      provides: "Reference doc with SQL queries for agent observability reporting"
    - path: "~/clawd/agents/main/observability.db"
      provides: "Backfilled baseline data from cron JSONL history"
  key_links:
    - from: "morning-briefing cron payload"
      to: "OBSERVABILITY.md reference doc"
      via: "New section 10 in cron systemEvent message"
    - from: "OBSERVABILITY.md SQL queries"
      to: "/workspace/observability.db"
      via: "sqlite3 CLI queries from sandbox"
    - from: "anomaly detection"
      to: "7-day rolling average"
      via: "SQL window function in OBSERVABILITY.md"
---

<objective>
Backfill baseline data from existing cron logs, create the observability reference doc with SQL queries, and integrate an Agent Observability section into the morning briefing.

Purpose: OBS-02 requires Bob to report per-agent metrics. OBS-03 requires the morning briefing to include anomaly detection. This plan wires the data (from Plan 01) into Bob's reporting workflow.

Output: OBSERVABILITY.md reference doc, morning briefing updated with section 10, baseline data backfilled, retention cleanup operational.
</objective>

<execution_context>
@/Users/andykaufman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/andykaufman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-agent-observability/26-RESEARCH.md
@.planning/phases/26-agent-observability/26-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Backfill baseline from cron JSONL and create OBSERVABILITY.md</name>
  <files>
    ~/clawd/agents/main/observability.db
    ~/clawd/agents/main/OBSERVABILITY.md
  </files>
  <action>
    SSH to EC2 (100.72.143.9).

    **1. Backfill from cron run JSONL:**
    Parse existing cron run logs at `~/.openclaw/cron/runs/*.jsonl` to populate observability.db with historical data. This jumpstarts the 7-day rolling average instead of waiting a full week.

    Write a Python script (run once, then delete) that:
    - Reads each `.jsonl` file in `~/.openclaw/cron/runs/`
    - For each line (JSON object), extract: `model`, `provider`, `usage.input_tokens`, `usage.output_tokens`, `durationMs`, `status`, timestamp from the run entry
    - Map the cron job ID to its agent from cron jobs.json (e.g., `863587f3` -> main, `review-check` -> sage)
    - Skip entries without `usage` data (heartbeats in main session have no usage)
    - INSERT into `llm_calls` table with backfill data (set `session_key` to `backfill:cron:<jobId>`)
    - INSERT into `agent_runs` table (success = status=='ok', duration_ms from durationMs)
    - Use estimated_cost_usd from pricing map (same rates as plugin: haiku $1/$5, sonnet $3/$15, opus $5/$25 per million)
    - Handle duplicates: check if row with same session_key and created_at already exists before INSERT

    Expected: 500-2000 historical rows from the ~2.2MB of JSONL data.

    **2. Add data retention cleanup:**
    Create a simple cron-compatible cleanup. Add a comment block at the top of OBSERVABILITY.md explaining that Bob should run cleanup weekly:
    ```sql
    DELETE FROM llm_calls WHERE created_at < datetime('now', '-90 days');
    DELETE FROM agent_runs WHERE created_at < datetime('now', '-90 days');
    ```
    This keeps observability.db under ~36MB max (90 days * 2000 rows/day * 200 bytes).

    **3. Create OBSERVABILITY.md reference doc:**
    Place at `~/clawd/agents/main/OBSERVABILITY.md`. This is the reference doc Bob reads during the morning briefing (like ANOMALY_ALERTS.md, MEETING_PREP.md, etc.).

    Contents:

    ```markdown
    ---
    name: agent-observability
    description: SQL queries and formatting for agent observability reporting
    ---

    # Agent Observability Reference

    ## Overview
    Query /workspace/observability.db to report on agent LLM usage, costs, latency, and anomalies.

    ## Section 10: Agent Observability (for Morning Briefing)

    ### 1. 24-Hour Summary Per Agent

    Run this SQL via `sqlite3 /workspace/observability.db`:

    [SQL from research: SELECT agent_id, COUNT(*) as turns, SUM(input_tokens), SUM(output_tokens), ROUND(SUM(estimated_cost_usd), 4), GROUP_CONCAT(DISTINCT model) FROM llm_calls WHERE created_at >= datetime('now', '-24 hours') GROUP BY agent_id ORDER BY cost_usd DESC]

    Format as a table with columns: Agent | Turns | Input Tokens | Output Tokens | Est. Cost | Models

    ### 2. Anomaly Detection

    [Full anomaly detection SQL from research with 7-day rolling average, 2x warning, 4x critical, zero_activity, collecting states]

    Interpretation:
    - `collecting`: Show "Baseline collecting (day N/7) — anomaly detection activates after 7 days of data"
    - `ok`: No flag needed
    - `warning`: Bold the agent row, prefix with WARNING
    - `critical`: Bold + highlight the agent row, prefix with CRITICAL
    - `zero_activity`: Show "WARNING: {agent} had zero activity (expected based on 7-day history)"

    ### 3. Error Summary (last 24h)

    [SQL: SELECT agent_id, COUNT(*) as error_count, last error message from agent_runs WHERE success=0]

    Show error count and most recent error message per agent. If zero errors, say "No errors in last 24h."

    ### 4. Average Latency Per Agent

    [SQL: SELECT agent_id, ROUND(AVG(duration_ms)/1000.0, 1) as avg_seconds FROM agent_runs WHERE created_at >= datetime('now', '-24 hours') GROUP BY agent_id]

    Format: Agent | Avg Response Time

    ### 5. Rate Limit Proximity (Estimated)

    Estimate rate limit usage from cumulative daily tokens:
    [SQL: SELECT SUM(input_tokens + output_tokens) as daily_tokens FROM llm_calls WHERE created_at >= datetime('now', '-24 hours')]

    Compare against known Anthropic tier limits. Flag if >80% of daily token budget.
    Note: This is an ESTIMATE based on token volume, not actual rate limit headers (headers are not available in hook events per research finding).

    ### 6. Data Retention
    Run weekly: DELETE FROM llm_calls WHERE created_at < datetime('now', '-90 days'); DELETE FROM agent_runs WHERE created_at < datetime('now', '-90 days');

    ## Formatting Guidelines
    - Two severity levels: **WARNING** (bold) and **CRITICAL** (bold + double asterisk prefix **)
    - If all agents are OK with no anomalies: "All agents operating within normal parameters."
    - During cold-start (< 7 days data): "Observability data collecting — baselines available after 7 days."
    - Include total estimated cost across all agents as a footer line
    ```

    Adjust the exact SQL to match the research file's verified queries. Include the full anomaly detection CTE from the research.
  </action>
  <verify>
    ```bash
    # Check backfill data exists
    ssh -i ~/.ssh/clawdbot-key.pem ubuntu@100.72.143.9 'sqlite3 /home/ubuntu/clawd/agents/main/observability.db "SELECT COUNT(*) as backfilled_rows FROM llm_calls WHERE session_key LIKE \"backfill:%\";"'

    # Check OBSERVABILITY.md exists
    ssh -i ~/.ssh/clawdbot-key.pem ubuntu@100.72.143.9 'test -f /home/ubuntu/clawd/agents/main/OBSERVABILITY.md && echo "OBSERVABILITY.md exists"'

    # Check distinct agents in backfill
    ssh -i ~/.ssh/clawdbot-key.pem ubuntu@100.72.143.9 'sqlite3 /home/ubuntu/clawd/agents/main/observability.db "SELECT DISTINCT agent_id FROM llm_calls ORDER BY agent_id;"'
    ```
  </verify>
  <done>observability.db contains backfilled historical data from cron JSONL (at least 100 rows), OBSERVABILITY.md exists at ~/clawd/agents/main/ with complete SQL queries for all 6 reporting sections</done>
</task>

<task type="auto">
  <name>Task 2: Add Agent Observability section to morning briefing cron</name>
  <files>
    ~/.openclaw/cron/jobs.json
  </files>
  <action>
    SSH to EC2 and update the morning briefing cron (ID: `863587f3-bb4e-409b-aee2-11fe2373e6e0`) to include the new observability section.

    **1. Read current morning briefing cron message:**
    ```bash
    python3 -c "
    import json
    with open('/home/ubuntu/.openclaw/cron/jobs.json') as f:
        jobs = json.load(f)
    for j in jobs:
        if j['id'] == '863587f3-bb4e-409b-aee2-11fe2373e6e0':
            print(j['message'][:500])
            print('...')
            break
    "
    ```

    **2. Add Section 10: Agent Observability**
    The morning briefing currently has 9 sections (per research: Sections 1-9 including AirSpace sections). Append a new section 10 to the cron message.

    Add to the end of the existing `message` field (before any closing instructions):

    ```
    ## 10. Agent Observability
    Read /workspace/OBSERVABILITY.md for SQL queries and formatting.
    Query /workspace/observability.db using sqlite3 to generate:
    1. 24h per-agent summary table (turns, tokens, cost, models used)
    2. Anomaly flags (warning at 2x 7-day avg, critical at 4x or repeated errors, zero-activity warning)
    3. Error summary (count + most recent error per agent, last 24h)
    4. Average latency per agent (last 24h)
    5. Rate limit proximity estimate (daily tokens vs tier limits)
    Format per OBSERVABILITY.md. If < 7 days of data exist, show cold-start message instead of anomaly flags.
    ```

    **3. Edit jobs.json:**
    Use Python to safely modify the JSON:
    - Read jobs.json
    - Find the morning briefing job by ID
    - Append the new section to its `message` field
    - Write back with proper JSON formatting
    - Do NOT change any other fields (schedule, model, delivery, etc.)

    **4. Restart gateway to pick up cron changes:**
    ```bash
    systemctl --user restart openclaw-gateway.service
    ```

    **5. Verify the briefing can produce observability data:**
    Test by manually running the morning briefing cron (this will take longer than a heartbeat — use 120s timeout):
    ```bash
    /home/ubuntu/.npm-global/bin/openclaw cron run 863587f3-bb4e-409b-aee2-11fe2373e6e0 --timeout 120000
    ```
    Check the run log to verify the observability section was generated:
    ```bash
    ls -t ~/.openclaw/cron/runs/863587f3*.jsonl | head -1 | xargs tail -1 | python3 -c "import sys,json; d=json.load(sys.stdin); print('status:', d.get('status')); print('usage:', d.get('usage',{}))"
    ```

    If the cron run succeeded (status: ok), the morning briefing now includes agent observability data.
  </action>
  <verify>
    ```bash
    # Verify section 10 is in the cron message
    ssh -i ~/.ssh/clawdbot-key.pem ubuntu@100.72.143.9 'python3 -c "
    import json
    with open(\"/home/ubuntu/.openclaw/cron/jobs.json\") as f:
        jobs = json.load(f)
    for j in jobs:
        if j[\"id\"] == \"863587f3-bb4e-409b-aee2-11fe2373e6e0\":
            print(\"Section 10 present:\", \"Agent Observability\" in j[\"message\"])
            break
    "'

    # Verify gateway is running
    ssh -i ~/.ssh/clawdbot-key.pem ubuntu@100.72.143.9 'systemctl --user is-active openclaw-gateway.service'

    # Check latest morning briefing run status
    ssh -i ~/.ssh/clawdbot-key.pem ubuntu@100.72.143.9 'ls -t ~/.openclaw/cron/runs/863587f3*.jsonl 2>/dev/null | head -1 | xargs tail -1 2>/dev/null | python3 -c "import sys,json; d=json.load(sys.stdin); print(\"status:\", d.get(\"status\"))" 2>/dev/null || echo "No run yet"'
    ```
  </verify>
  <done>Morning briefing cron message includes "## 10. Agent Observability" section, gateway is running, and a test run of the morning briefing completes successfully (status: ok) with observability data in the output</done>
</task>

</tasks>

<verification>
1. `sqlite3 observability.db "SELECT COUNT(*) FROM llm_calls WHERE session_key LIKE 'backfill:%'"` — backfill data exists (>= 100 rows)
2. `test -f ~/clawd/agents/main/OBSERVABILITY.md` — reference doc exists
3. Morning briefing cron message contains "Agent Observability" section
4. Morning briefing test run completes with status: ok
5. `sqlite3 observability.db "SELECT DISTINCT agent_id FROM llm_calls"` — multiple agents represented
6. Gateway service is active after all changes
</verification>

<success_criteria>
- Backfilled data from cron JSONL provides initial baseline (multiple agents, multiple days)
- OBSERVABILITY.md has complete SQL queries for all 6 reporting areas (summary, anomaly, errors, latency, rate limit, retention)
- Morning briefing cron includes Section 10 with observability instructions
- Test morning briefing run produces observability section in output
- Cold-start handling works (shows "collecting baseline" message when < 7 days)
- Anomaly detection logic distinguishes warning (2x) from critical (4x) severity
</success_criteria>

<output>
After completion, create `.planning/phases/26-agent-observability/26-02-SUMMARY.md`
</output>
