---
phase: 18-pipeline-monitoring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - /home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md
  - /home/ubuntu/.openclaw/cron/jobs.json
autonomous: true

must_haves:
  truths:
    - "Sentinel generates a weekly content pipeline report with topic/article/social stats"
    - "Report includes pipeline velocity (items moved through each stage)"
    - "Report posts to #ops channel every Sunday"
  artifacts:
    - path: "/home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md"
      provides: "Reference doc with SQL queries for weekly pipeline stats"
    - path: "/home/ubuntu/.openclaw/cron/jobs.json"
      provides: "pipeline-report cron entry"
  key_links:
    - from: "pipeline-report cron"
      to: "PIPELINE_REPORT.md"
      via: "Sentinel reads reference doc on cron trigger"
    - from: "PIPELINE_REPORT.md SQL queries"
      to: "/home/ubuntu/clawd/content.db"
      via: "sqlite3 CLI queries in embedded mode (host paths)"
---

<objective>
Deploy a weekly content pipeline report to Sentinel (ops agent) that summarizes pipeline health: topic backlog depth, article statuses, publishing throughput, social post generation, and overall pipeline velocity.

Purpose: MN-01 (weekly report) + MN-03 (Sentinel integration) — give the operator visibility into content pipeline throughput without manual DB queries.
Output: PIPELINE_REPORT.md reference doc on EC2 + weekly cron job targeting ops.
</objective>

<execution_context>
@/Users/andykaufman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/andykaufman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PIPELINE_REPORT.md reference doc for Sentinel</name>
  <files>/home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md</files>
  <action>
SSH to EC2 (100.72.143.9) and create /home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md with the following sections:

**Section 1: Weekly Pipeline Report Instructions**
- Agent reads this doc when triggered by weekly pipeline-report cron
- Generate report covering the past 7 days
- Post formatted summary to #ops channel (not DM)

**Section 2: Topic Pipeline Stats**
SQL queries using sqlite3 /home/ubuntu/clawd/content.db (HOST path, not /workspace/ — embedded mode):
```sql
-- Topics by status
SELECT status, COUNT(*) FROM topics GROUP BY status;

-- Topics created this week
SELECT COUNT(*) FROM topics WHERE created_at >= datetime('now', '-7 days');

-- Topics claimed but not yet written (potential bottleneck)
SELECT COUNT(*) FROM topics WHERE status = 'researching' OR (status = 'writing' AND claimed_by IS NOT NULL);
```

**Section 3: Article Pipeline Stats**
```sql
-- Articles by status
SELECT status, COUNT(*) FROM articles GROUP BY status;

-- Articles completed this week (moved to approved or published)
SELECT COUNT(*) FROM articles WHERE status IN ('approved', 'published') AND updated_at >= datetime('now', '-7 days');

-- Average review scores for articles reviewed this week
SELECT ROUND(AVG(seo_score),1) as avg_seo, ROUND(AVG(readability_score),1) as avg_read, ROUND(AVG(accuracy_score),1) as avg_acc FROM articles WHERE seo_score IS NOT NULL AND updated_at >= datetime('now', '-7 days');
```

**Section 4: Publishing & Social Stats**
```sql
-- Articles published this week
SELECT COUNT(*) FROM articles WHERE status = 'published' AND published_at >= datetime('now', '-7 days');

-- Social posts generated this week
SELECT platform, COUNT(*) FROM social_posts WHERE created_at >= datetime('now', '-7 days') GROUP BY platform;

-- Social posts by status
SELECT status, COUNT(*) FROM social_posts GROUP BY status;
```

**Section 5: Pipeline Velocity (activity log)**
```sql
-- Actions by agent this week
SELECT agent_id, action, COUNT(*) FROM pipeline_activity WHERE created_at >= datetime('now', '-7 days') GROUP BY agent_id, action;

-- Status transitions this week
SELECT entity_type, old_status, new_status, COUNT(*) FROM pipeline_activity WHERE created_at >= datetime('now', '-7 days') AND old_status IS NOT NULL GROUP BY entity_type, old_status, new_status;
```

**Section 6: Report Format**
Format as Slack message with:
- Header: "Content Pipeline Weekly Report (DATE_RANGE)"
- Subsections: Topic Backlog, Article Progress, Publishing, Social, Velocity
- Use emoji sparingly (checkmark for published, warning for bottlenecks)
- End with 1-2 sentence health summary

**Section 7: Important Notes**
- This runs in EMBEDDED mode — use host paths, not /workspace/
- DB path: /home/ubuntu/clawd/content.db
- Post to #ops channel
- If no activity this week, still post a brief "no pipeline activity" summary
  </action>
  <verify>
SSH verify: `cat /home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md | wc -l` returns 80+ lines.
SSH verify: `grep -c 'content.db' /home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md` returns 5+ (host path references).
SSH verify: `grep 'workspace' /home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md` returns 0 matches (no sandbox paths).
  </verify>
  <done>PIPELINE_REPORT.md deployed to Sentinel's workspace with SQL queries covering all 5 reporting areas, using host paths for embedded mode execution.</done>
</task>

<task type="auto">
  <name>Task 2: Create weekly pipeline-report cron job for Sentinel</name>
  <files>/home/ubuntu/.openclaw/cron/jobs.json</files>
  <action>
SSH to EC2 and create a weekly cron job:

```bash
/home/ubuntu/.npm-global/bin/openclaw cron add \
  --name "pipeline-report" \
  --cron "0 16 * * 0" \
  --message "Read /home/ubuntu/clawd/agents/ops/PIPELINE_REPORT.md and generate the weekly content pipeline report. Query content.db at /home/ubuntu/clawd/content.db using sqlite3 CLI. Post the formatted report to #ops." \
  --timeout 120000
```

Schedule: Sunday at 16:00 UTC = 8 AM PT (DST-safe with tz field if available, otherwise raw UTC is fine for weekly job).

After CLI creates the job, patch jobs.json to set:
- `sessionTarget`: "ops" (CLI only supports main|isolated, must patch directly)
- `payload.kind`: "agentTurn" (must match sessionTarget=ops)
- `payload.model`: "sonnet" (content agent cron pattern)
- Remove any `delivery` config (not needed for non-isolated sessions)

Restart gateway after patching: `systemctl --user restart openclaw-gateway.service`

Verify the job appears in cron list and has correct config.
  </action>
  <verify>
SSH verify: `/home/ubuntu/.npm-global/bin/openclaw cron list | grep pipeline-report` shows the job.
SSH verify: `cat /home/ubuntu/.openclaw/cron/jobs.json | python3 -c "import sys,json; jobs=json.load(sys.stdin); pr=[j for j in jobs if j.get('name')=='pipeline-report']; print(pr[0]['sessionTarget'] if pr else 'NOT FOUND')"` returns "ops".
SSH verify: `systemctl --user is-active openclaw-gateway.service` returns "active".
  </verify>
  <done>pipeline-report cron job fires every Sunday at 8 AM PT targeting Sentinel (ops), using agentTurn with sonnet model, reading PIPELINE_REPORT.md for SQL query instructions.</done>
</task>

</tasks>

<verification>
- PIPELINE_REPORT.md exists on EC2 at /home/ubuntu/clawd/agents/ops/ with SQL queries for all 5 reporting areas
- pipeline-report cron job exists in jobs.json with sessionTarget=ops, kind=agentTurn, model=sonnet
- Gateway is active after restart
- No /workspace/ paths in PIPELINE_REPORT.md (embedded mode compliance)
- Manual trigger: `openclaw cron run pipeline-report --timeout 120000` produces a report (optional, depends on having data in content.db)
</verification>

<success_criteria>
Weekly pipeline report cron is configured and will fire every Sunday, targeting Sentinel with instructions to query content.db and post stats to #ops.
</success_criteria>

<output>
After completion, create `.planning/phases/18-pipeline-monitoring/18-01-SUMMARY.md`
</output>
